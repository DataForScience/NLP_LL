{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> \n",
    "        <h1>Natural Language Processing</h1>\n",
    "        <h1>Applications</h1>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "        @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import gzip\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "\n",
    "import watermark\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List out the versions of all loaded libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('./d4sci.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading a well trained set of word embeddings from project polyglot: https://sites.google.com/site/rmyeid/projects/polyglot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, embeddings = pd.read_pickle('data/polyglot-en.pkl')\n",
    "embeddings = normalize(embeddings)\n",
    "word_list = np.array(word_list)\n",
    "word_dict = dict(zip(word_list, range(embeddings.shape[0])))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, embeddings, dictionary, reverse_dictionary, top_k=8):   \n",
    "    valid_word = dictionary[word]\n",
    "    similarity = cosine_similarity(embeddings, embeddings[valid_word, :].reshape(1, -1))\n",
    "    nearest = (-similarity).argsort(axis=0)[1:top_k + 1].flatten()\n",
    "    return reverse_dictionary[nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"King\", embeddings, word_dict, word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question task set downloaded from: http://download.tensorflow.org/data/questions-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('data/questions-words.txt', comment=':', sep=' ', header=None)\n",
    "print(questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a function to automatically evaluate this specific type of analogy. We simply look up the embeddings for each of the four words in the question and perform the necessary vector algebra. To be safe, we enclose the entire function into a try/except block to catch the exceptions thrown when we try to use a word that is not part of our vocabulary (included in the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogy(question):\n",
    "    word1, word2, word3, word4 = question\n",
    "\n",
    "    if word1 not in word_dict or \\\n",
    "       word2 not in word_dict or \\\n",
    "       word3 not in word_dict or \\\n",
    "       word4 not in word_dict:\n",
    "        return None\n",
    "\n",
    "    key1 = word_dict[word1]\n",
    "    key2 = word_dict[word2]\n",
    "    key3 = word_dict[word3]\n",
    "    key4 = word_dict[word4]\n",
    "\n",
    "    vec1 = embeddings[key1, :]\n",
    "    vec2 = embeddings[key2, :]\n",
    "    vec3 = embeddings[key3, :]\n",
    "    vec4 = embeddings[key4, :]\n",
    "\n",
    "    predict = vec2-vec1+vec3\n",
    "\n",
    "    sim = np.matmul(predict, embeddings.T)\n",
    "    nearest = np.argsort(-sim)[:10]\n",
    "\n",
    "    return word4 in word_list[nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_analogy(questions.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [evaluate_analogy(questions.iloc[i]) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_results = [res for res in results if res is not None]\n",
    "accuracy = np.mean(clean_results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We this simple approach we achieve ~53% accuracy. Our results are penalized by the fact that our embeddings weren't generated specifically for this purpose and are missing some of the words used in the analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(embeddings.T, aspect=700, cmap=cm.seismic)\n",
    "plt.grid(None)\n",
    "plt.xlabel(\"vocabulary\")\n",
    "plt.ylabel(\"dimensions\")\n",
    "plt.colorbar(orientation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dimensionality reduction algorithm like t-SNE we are able to project our word embeddings into a two dimensional space for visualization purposes. While the details of how t-SNE does its magic are beyond the scope of this course, the fundamental idea is the same of algorithms like PCA or the matrix decomposition methods we explored. Project the dataset into a latent, and lower dimensional, space in a smart way. t-SNE became popular in recent year due to its hability to make projections into 2D space in a way that preserves as much of the higher dimensional structure as possible resulting in beautiful and useful visualizations. \n",
    "\n",
    "The start implementation of t-SNE is that that comes bundled with **sklearn**. Here we simply call it to generate the plot we saw in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "plot_only = 500 # Plot only 500 words\n",
    "low_dim_embs = tsne.fit_transform(np.array(embeddings)[:plot_only, :])\n",
    "labels = [word_list[i] for i in range(plot_only)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word co-occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe relies on the word co-occurences matrix. Let's take a look in detail on how to calculate it. For convenience, we'll use the same nursery rhyme we looked at in Lesson I. To save time we'll just load the data from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"data/mary.pickle\", \"rb\"))\n",
    "mary_word_list = data['word_list']\n",
    "mary_word_dict = data['word_dict']\n",
    "text_words = data['text_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that everything looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in mary_word_dict:\n",
    "    if mary_word_list[mary_word_dict[word]] != word:\n",
    "        print(\"ERROR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that we got the right text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to define a function that calculates the cooccurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurence_matrix(word_dict, text_words, window_size=1):\n",
    "    vocabulary_size = len(word_dict)\n",
    "    matrix = np.zeros((vocabulary_size, vocabulary_size), dtype='int')\n",
    "\n",
    "    for i in range(window_size+1, len(text_words)-window_size):\n",
    "        word_id = word_dict[text_words[i]]\n",
    "        \n",
    "        for j in range(i-window_size, i+window_size+1):\n",
    "            if j == i: \n",
    "                continue\n",
    "            \n",
    "            context_id = word_dict[text_words[j]]\n",
    "            \n",
    "            matrix[word_id, context_id] += 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what this matrix looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = cooccurence_matrix(mary_word_dict, text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the matrix elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = matrix.astype('str')\n",
    "temp[temp=='0'] = \"\"\n",
    "pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it might appear symmetrical, that is actually not true, as we can easily see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(matrix-matrix.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or more directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix[mary_word_dict['a'], mary_word_dict['had']])\n",
    "print(matrix[mary_word_dict['had'], mary_word_dict['a']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common implementations of GloVe scale the contribution of each context word by its distance to the center word. In this way, nearer words contribute more than more distant ones. Using this unweighted definition, we can also calculate the conditional probability $P(w|C)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob = matrix/matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example will confirm that this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob[mary_word_dict['mary'], mary_word_dict['had']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that 1/3. of the occurences of the word mary are next to the word 'had'. Let's confirm it explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_words)):\n",
    "    if text_words[i] == 'mary':\n",
    "        if i != 0:\n",
    "            print(text_words[i-1], text_words[i], text_words[i+1])\n",
    "        else:\n",
    "            print(None, text_words[i], text_words[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the complementary probability $P(had|mary)$ is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob[mary_word_dict['had'], mary_word_dict['mary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, GloVe relies on this coocurrence matrix to define its embeddings. We can also easily see how more complex language models of the form $P(word|word_1,word_2, \\cdots)$ can be obtained by changing the way in which the columns are defined (and increasing their number significantly). \n",
    "\n",
    "Surprisingly, in 2014, it was shown by O. Levy and Y. Goldberg in a highly cited [NIPS paper](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) that some variations of word2vec are equivalent to factorizing a word-context matrix. These two approches for obtaining word embeddings aren't so differnet after all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building a vector of character distributions for each language. Due to the total size of the google books dataset, we include only a partial file in the data directory. The interested student is encouraged to download all teh files and use the code below to build hers or his own language detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(string.ascii_letters.lower()))\n",
    "\n",
    "dict_char = dict(zip(characters, range(len(characters))))\n",
    "counts = np.zeros(len(characters), dtype='uint64')\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "filename = \"data/googlebooks-eng-all-1gram-20120701-a.gz\"\n",
    "\n",
    "for line in gzip.open(filename, \"rt\"):\n",
    "    fields = line.lower().strip().split()\n",
    "\n",
    "    line_count += 1\n",
    "\n",
    "    if line_count % 100000 == 0:\n",
    "        print(filename, line_count)\n",
    "        break\n",
    "\n",
    "    count = int(fields[2])\n",
    "    word = fields[0]\n",
    "\n",
    "    if \"_\" in word:\n",
    "        continue\n",
    "\n",
    "    letters = [char for char in word if char in characters]\n",
    "\n",
    "    if len(letters) != len(word):\n",
    "        continue\n",
    "\n",
    "    for letter in letters:\n",
    "        if letter not in dict_char:\n",
    "            continue\n",
    "\n",
    "        counts[dict_char[letter]] += count\n",
    "\n",
    "total = np.sum(counts)\n",
    "list_char = list(dict_char.items())\n",
    "list_char.sort(key=lambda x: x[1])\n",
    "\n",
    "for key, value in enumerate(list_char):\n",
    "    print(value[0], counts[key]/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the most common character is the letter *a*. This is an artifact of the fact that we are using the datafile containing only words that start with the letter *a*. If you were to run it on the entire dataset, the resutls shown in the slides would be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, I've also included the complete table for all 5 languages in the repository. This is the datset that we will used to build our language detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making a quick visualization of the probabiltity distributions for each language. THe first step is to load up the language character frequency from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_letter_lang = pd.read_csv('data/table_langs.dat', sep=' ', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram for the English language character frequency is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(26), P_letter_lang[\"eng\"])\n",
    "plt.xticks(list(range(26)), P_letter_lang.index)\n",
    "plt.xlabel(\"letter\")\n",
    "plt.ylabel(\"P(letter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can easily find significant differences between the various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = P_letter_lang.plot()\n",
    "ax.set_xticks(list(range(26)))\n",
    "ax.set_xticklabels(P_letter_lang.index)\n",
    "ax.legend([\"English\", \"French\", \"German\", \"Italian\", \"Spanish\"])\n",
    "ax.set_xlabel(\"letter\")\n",
    "ax.set_ylabel(\"P(letter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there definitely some common trends (the letters *q* and *j* are underrepreented across all languages), there are also some significant peaks that will help us discriminate between one language and the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this table of data it is extremely simple to build a Naive Bayes classifier. To do so, one must just calculte the correct set of log likelihoods so that we may use them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(P_letter_lang):\n",
    "    langs = list(P_letter_lang.columns)\n",
    "\n",
    "    P_letter = P_letter_lang.mean(axis=1)\n",
    "    P_letter /= P_letter.sum()\n",
    "\n",
    "    P_lang_letter = np.array(P_letter_lang)/(P_letter_lang.shape[1]*P_letter.values.reshape(-1, 1))\n",
    "\n",
    "    L_lang_letter = np.log(P_lang_letter.T)\n",
    "\n",
    "    return langs, P_letter, L_lang_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs, P_letter, L_lang_letter = process_data(P_letter_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have all the tools we need to write down our mini detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(langs, P_letter, L_lang_letter, text):\n",
    "    counts = np.zeros(26, dtype='int')\n",
    "    pos = dict(zip(P_letter.index, range(26)))\n",
    "\n",
    "    text_counts =  Counter(text).items()\n",
    "\n",
    "    for letter, count in text_counts:\n",
    "        if letter in pos:\n",
    "            counts[pos[letter]] += count\n",
    "\n",
    "    L_text = np.dot(L_lang_letter, counts)\n",
    "    index = np.argmax(L_text)\n",
    "    lang_text = langs[index]\n",
    "    prob = np.exp(L_text[index])/np.sum(np.exp(L_text))*100\n",
    "\n",
    "    return lang_text, prob, L_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it. So now let's test our detector with a few past headlines from Google News:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "texts[\"eng\"] = \"North Korea‚Äôs Test of Nuclear Bomb Amplifies a Global Crisis\".lower()\n",
    "texts[\"ita\"] = \"Nucleare, Onu riunisce consiglio sicurezza. E Seul simula attacco alle basi di Kim\".lower()\n",
    "texts[\"fre\"] = \"Cor√©e du Nord : les Etats-Unis pr√™ts √† utiliser leurs capacit√©s nucl√©aires\".lower()\n",
    "texts[\"spa\"] = \"Estados Unidos amenaza con una ‚Äúrespuesta militar masiva‚Äù a Corea del Norte\".lower()\n",
    "texts[\"ger\"] = \"√úberraschung\".lower()\n",
    "texts[\"ita2\"] = \"Wales lancia la Wikipedia delle news. Contro il fake in campo anche Google\".lower()\n",
    "\n",
    "for lang in texts:\n",
    "    text = texts[lang]\n",
    "    lang_text, prob, L_text = detect_lang(langs, P_letter, L_lang_letter, text)\n",
    "    print(lang, lang_text, prob, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we do a pretty good job. We get 5/6 correct and the only one we are missing is a specific case where there are a surprising number of English words in the middle of an Italian headline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
